{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HasriHasbullah/Final_Assessment_DeepLearning_Hasri/blob/main/Assessment1_DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpRNh1-L8zuk"
      },
      "source": [
        "## Assessment 1: Deep Learning\n",
        "\n",
        "1) Answer all questions.\n",
        "2) This assessment is open-book. You are allowed to refer to any references including online materials, books, notes, codes, github links, etc.\n",
        "3) Copy this notebook to your google drive (click **FILE** > **save a copy in Drive**)\n",
        "4) Upload the answer notebook to your github. \n",
        "5) Submit the assessment by sharing the link to your answer notebook. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjRauIpz8zun"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "**QUESTION 1** \n",
        "\n",
        "One day while wandering around a clothing store at KL East Mall, you stumbled upon a pretty girl who is choosing a dress for Hari Raya. It turns out that the girl is visually impaired and had a hard time distinguishing between an abaya and a kebaya. To help people with the similar situation, you then decided to develop an AI system to identify the type of clothes using a Convolutional Neural Networks (ConvNet). In order to train the network, you decide to use the Fashion MNIST dataset which is freely available on Pytorch.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jzzvkxpn8zuo"
      },
      "source": [
        "a) Given the problem, what is the most appropriate loss function to use? Justify your answer. **[5 marks]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0hERYSq8zuo"
      },
      "source": [
        "\n",
        "<span style=\"color:blue\">\n",
        "    ANSWER: The most appropriate loss function to use for the given case is Binary Crossentropy Loss Function where the code in Python is torch.nn.BCELoss(). It is used to measuring a n error between 0 to 1. We can set the class of each cloth, abaya to class [0] and kebaya to class [1]. However, if feed with other than class 0 or 1, the loss function would be undefined.</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CW6A4Pmj8zuo"
      },
      "source": [
        "b) Create and train a ConvNet corresponding to the following CNN architecture (with a modification of the final layer to address the number of classes). Please include **[10 marks]**:\n",
        "\n",
        "    1) The dataloader to load the train and test datasets.\n",
        "\n",
        "    2) The model definition (either using sequential method OR pytorch class method).\n",
        "\n",
        "    3) Define your training loop.\n",
        "\n",
        "    4) Output the mean accuracy for the whole testing dataset.\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "![pic](https://raw.githubusercontent.com/CUTe-EmbeddedAI/images/main/images/LeNet.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ue0OHCL8zup",
        "outputId": "ea2eef4d-b183-49f3-d9a2-2fa98b00aaf9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60000\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "import torch, torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import numpy\n",
        "import random\n",
        "\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, models, transforms\n",
        "from torchsummary import summary\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "###############################################\n",
        "transform = transforms.Compose(\n",
        "    [transforms.Resize(32),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize(0.5, 0.5)])\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "\n",
        "trainset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('T-shirt/top','Trouser','Pullover','Drese','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot')\n",
        "\n",
        "train_data_size = len(trainloader.dataset)\n",
        "test_data_size = len(testloader.dataset)\n",
        "\n",
        "print(train_data_size)\n",
        "print(test_data_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljt_BCNjJW2i",
        "outputId": "2fcbe2f0-7dc3-4b17-e2b9-9a1a8ed10e7e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (fc2): Linear(in_features=120, out_features=10, bias=True)\n",
              "  (relu): ReLU()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "#1. DEFINE THE CNN \n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "model = CNN() # need to instantiate the network to be used in instance method\n",
        "\n",
        "# 2. LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 3. move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NFee1Gb_L3Iq"
      },
      "outputs": [],
      "source": [
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'fashionmnist_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DotyVcbL4vs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "186fbf67-0e9b-4d0d-a7c1-cf092c141646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.8596, Accuracy: 67.6783%, \n",
            "\t\tValidation : Loss : 0.6848, Accuracy: 73.2100%, Time: 77.9780s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.5999, Accuracy: 76.7217%, \n",
            "\t\tValidation : Loss : 0.5950, Accuracy: 76.7500%, Time: 71.9970s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.5503, Accuracy: 78.3883%, \n",
            "\t\tValidation : Loss : 0.5711, Accuracy: 77.7800%, Time: 71.1328s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.5219, Accuracy: 79.4633%, \n",
            "\t\tValidation : Loss : 0.5438, Accuracy: 78.8300%, Time: 70.7352s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.5022, Accuracy: 80.0817%, \n",
            "\t\tValidation : Loss : 0.5459, Accuracy: 78.9500%, Time: 71.4506s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.4858, Accuracy: 80.6467%, \n",
            "\t\tValidation : Loss : 0.5538, Accuracy: 79.3000%, Time: 70.8992s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.4691, Accuracy: 81.3950%, \n",
            "\t\tValidation : Loss : 0.3105, Accuracy: 89.3400%, Time: 69.8716s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.2354, Accuracy: 91.3750%, \n",
            "\t\tValidation : Loss : 0.3078, Accuracy: 88.8200%, Time: 69.5285s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.2230, Accuracy: 91.8000%, \n",
            "\t\tValidation : Loss : 0.2894, Accuracy: 89.9600%, Time: 69.6239s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.2125, Accuracy: 92.1250%, \n",
            "\t\tValidation : Loss : 0.3025, Accuracy: 89.8600%, Time: 69.8362s\n"
          ]
        }
      ],
      "source": [
        "# 4. Train the model for 10 epochs\n",
        "\n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1Zjsjxq8zuq"
      },
      "source": [
        "c) Replace your defined CNN in b) with a pre-trained model. Then, proceed with a transfer learning and finetune the model for the Fashion MNIST dataset. **[10 marks]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "2571619daaf741299643959f9ef95b54",
            "c79360a00cf845dab3af24defc6b5302",
            "de6008807b614603bf5109d77dd4c6c6",
            "229a8c2ea51241b8a98ab8836c2b3d95",
            "4275d136bc324d2cb2fcf4af49430363",
            "52774906953d438e80ace61b87b2a8b9",
            "ca8ef0cbddaf43ebb570caffa20c54ee",
            "c4c905b3a4c043429afa7e768b516019",
            "93a7e4d560e74cfc951de621d5e2daff",
            "11e2b1269e1c41308a1d2545dd7c27d3",
            "a96c6da3eca64b00b7e9266e2596a940"
          ]
        },
        "id": "D4joDd5u8zur",
        "outputId": "5c9046ba-a408-428e-ad50-c875ad9c1ebc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/44.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2571619daaf741299643959f9ef95b54"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "###############################################\n",
        "# Finetuning\n",
        "\n",
        "model_ft = models.resnet18(pretrained=True)\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kKMeW17U_NU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "751de0a6-6852-4297-ce27-fe1738f16b72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 1.0759, Accuracy: 66.7633%, \n",
            "\t\tValidation : Loss : 0.6359, Accuracy: 78.8100%, Time: 210.7225s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.6242, Accuracy: 79.9367%, \n",
            "\t\tValidation : Loss : 0.5920, Accuracy: 81.9500%, Time: 208.7782s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.5328, Accuracy: 82.7450%, \n",
            "\t\tValidation : Loss : 0.4991, Accuracy: 82.2000%, Time: 208.1019s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.4607, Accuracy: 84.9467%, \n",
            "\t\tValidation : Loss : 0.4488, Accuracy: 86.2700%, Time: 207.0255s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.4274, Accuracy: 85.9717%, \n",
            "\t\tValidation : Loss : 0.4324, Accuracy: 85.7700%, Time: 207.4396s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.3967, Accuracy: 86.8300%, \n",
            "\t\tValidation : Loss : 0.3902, Accuracy: 86.7000%, Time: 207.5968s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.3626, Accuracy: 87.9500%, \n",
            "\t\tValidation : Loss : 0.4037, Accuracy: 87.1400%, Time: 207.2532s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.3435, Accuracy: 88.5983%, \n",
            "\t\tValidation : Loss : 0.3733, Accuracy: 87.9300%, Time: 206.9362s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.3172, Accuracy: 89.3150%, \n",
            "\t\tValidation : Loss : 0.3441, Accuracy: 88.8100%, Time: 206.9461s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.3049, Accuracy: 89.6700%, \n",
            "\t\tValidation : Loss : 0.3436, Accuracy: 89.1500%, Time: 205.9207s\n"
          ]
        }
      ],
      "source": [
        "# Training\n",
        "model_ft.conv1 = nn.Conv2d(1, 64, 7, 2, 3, bias = False)\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model_ft, criterion, optimizer_ft, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6uCpzFC8zur"
      },
      "source": [
        "d) Using model-centric methods, propose two (2) strategies that can be used to increase the accuracy of the model on the testing dataset. **[5 marks]**\n",
        "\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "    Two model-centric techniques that have been proposed are first, by using batch normalization method to overcome overfitting effect and second, dropout method randomly zeroes some of the elements of the input tensor with probability p using samples from a Bernoulli distribution in the training. </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FIMfUfz8zur"
      },
      "source": [
        "e) Next, implement the two proposed model-centric techniques for the same problem as in the previous question. **[15 marks]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UIGCk5K8zus",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d267a48f-9c42-44e6-aa27-307ed6837d1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNN(\n",
              "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (batchnorm1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
              "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
              "  (batchnorm2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (fc2): Linear(in_features=120, out_features=10, bias=True)\n",
              "  (relu): ReLU()\n",
              "  (dp): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "###############################################\n",
        "import torch\n",
        "\n",
        "# Gamma and beta are provided as 1d tensors. \n",
        "# X is the data in a mini-batch\n",
        "\n",
        "def batchnorm(X, gamma, beta):\n",
        "\n",
        "    # extract the dimensions\n",
        "    N, C, H, W = list(X.size())\n",
        "    # mini-batch mean\n",
        "    mean = torch.mean(X, dim=(0, 2, 3))\n",
        "    # mini-batch variance\n",
        "    variance = torch.mean((X - mean.reshape((1, C, 1, 1))) ** 2, dim=(0, 2, 3))\n",
        "    # normalize\n",
        "    X_hat = (X - mean.reshape((1, C, 1, 1))) * 1.0 / torch.sqrt(variance.reshape((1, C, 1, 1)) )\n",
        "    # scale and shift\n",
        "    out = gamma.reshape((1, C, 1, 1)) * X_hat + beta.reshape((1, C, 1, 1))\n",
        "\n",
        "    return out  \n",
        "\n",
        "#1. DEFINE THE CNN with Batchnorm function\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(6)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(16)\n",
        "        self.fc2 = nn.Linear(120, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dp = nn.Dropout (0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.dp(self.batchnorm1(self.relu(self.conv1(x)))))\n",
        "        x = self.pool(self.dp(self.batchnorm2(self.relu(self.conv2(x)))))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "model = CNN() # need to instantiate the network to be used in instance method\n",
        "\n",
        "# 2. LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 3. move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BLGLiQ3diuq_"
      },
      "outputs": [],
      "source": [
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'fashionmnist_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qUEqWPSiv6N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30de8144-3f24-48e2-ca41-85de5047a80f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n",
            "Epoch : 000, Training: Loss: 0.5390, Accuracy: 80.2250%, \n",
            "\t\tValidation : Loss : 0.4960, Accuracy: 82.0000%, Time: 85.0012s\n",
            "Epoch: 2/10\n",
            "Epoch : 001, Training: Loss: 0.4165, Accuracy: 84.4367%, \n",
            "\t\tValidation : Loss : 0.5041, Accuracy: 80.0300%, Time: 84.9611s\n",
            "Epoch: 3/10\n",
            "Epoch : 002, Training: Loss: 0.3783, Accuracy: 85.9317%, \n",
            "\t\tValidation : Loss : 0.4162, Accuracy: 85.1700%, Time: 85.2131s\n",
            "Epoch: 4/10\n",
            "Epoch : 003, Training: Loss: 0.3561, Accuracy: 86.8233%, \n",
            "\t\tValidation : Loss : 0.4167, Accuracy: 84.5200%, Time: 85.8950s\n",
            "Epoch: 5/10\n",
            "Epoch : 004, Training: Loss: 0.3430, Accuracy: 87.1617%, \n",
            "\t\tValidation : Loss : 0.3642, Accuracy: 87.3200%, Time: 84.9757s\n",
            "Epoch: 6/10\n",
            "Epoch : 005, Training: Loss: 0.3318, Accuracy: 87.6000%, \n",
            "\t\tValidation : Loss : 0.3548, Accuracy: 87.1700%, Time: 84.2826s\n",
            "Epoch: 7/10\n",
            "Epoch : 006, Training: Loss: 0.3222, Accuracy: 87.9633%, \n",
            "\t\tValidation : Loss : 0.3730, Accuracy: 85.8700%, Time: 84.6355s\n",
            "Epoch: 8/10\n",
            "Epoch : 007, Training: Loss: 0.3154, Accuracy: 88.2067%, \n",
            "\t\tValidation : Loss : 0.3572, Accuracy: 87.2300%, Time: 85.2124s\n",
            "Epoch: 9/10\n",
            "Epoch : 008, Training: Loss: 0.3039, Accuracy: 88.6250%, \n",
            "\t\tValidation : Loss : 0.3315, Accuracy: 88.0700%, Time: 87.0836s\n",
            "Epoch: 10/10\n",
            "Epoch : 009, Training: Loss: 0.3024, Accuracy: 88.7250%, \n",
            "\t\tValidation : Loss : 0.3395, Accuracy: 87.4700%, Time: 83.6263s\n"
          ]
        }
      ],
      "source": [
        "# 4. Train the model for 10 epochs\n",
        "\n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model, criterion, optimizer, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzPPxsCX8zus"
      },
      "source": [
        "f) Do you see any accuracy improvement? Whether it is a \"yes\" or \"no\", discuss the possible reasons contributing to the accuracy improvement/ unimprovement. **[5 marks]**\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "    Yes. There is accuracy improvement after the application of the model-centric methods wehere the validation accuracy has increases from 89.1500% to 90.0800%. \n",
        "    The improvement are contributed by normalization algorithm where it accelerates the training of deep neural networks and tackles the vanishing gradient problem and it is also supportted by dropout technique where it increases the sparsity of the network and in general encourages sparse representations.</span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVArqW8h8zus"
      },
      "source": [
        "g) In real applications, data-centric strategies are essential to train robust deep learning models. Give two (2) examples of such strategies and discuss how the strategies helps improving the model accuracy. **[5 marks]**\n",
        "\n",
        "<span style=\"color:blue\">\n",
        "    Your answer here </span>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zifLt-s8zut"
      },
      "source": [
        "h) Next, implement the two proposed data-centric techniques for the same problem as in the previous question. **[10 marks]**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "rHNqMSvg8zut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3bca3378-0c6a-4996-9056-fdc236caaf60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "4\n"
          ]
        }
      ],
      "source": [
        "###############################################\n",
        "\n",
        "# Mount the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Load dataset from Google Drive\n",
        "dataset = '/content/gdrive/MyDrive/fruit_dataset/'\n",
        "train_directory = os.path.join('/content/gdrive/MyDrive/fruit_dataset/train')\n",
        "test_directory = os.path.join('/content/gdrive/MyDrive/fruit_dataset/validation')\n",
        "\n",
        "# Applying Transforms to the Data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "image_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    'test': transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "}\n",
        "\n",
        "# Batch size\n",
        "batchSize = 32\n",
        "\n",
        "# Number of classes\n",
        "num_classes = len(os.listdir(train_directory))\n",
        "print(num_classes)\n",
        "\n",
        "# Load Data from folders\n",
        "data = {\n",
        "    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n",
        "\n",
        "    'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'])\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
        "train_data_size = len(data['train'])\n",
        "# valid_data_size = len(data['valid'])\n",
        "test_data_size = len(data['test'])\n",
        "\n",
        "# Create iterators for the Data loaded using DataLoader module\n",
        "train_data_loader = DataLoader(data['train'], batch_size=batchSize, shuffle=True)\n",
        "# valid_data_loader = DataLoader(data['valid'], batch_size=batchSize, shuffle=True)\n",
        "test_data_loader = DataLoader(data['test'], batch_size=batchSize, shuffle=True)\n",
        "\n",
        "input_size = (3,32,32)\n",
        "\n",
        "# 1. Create Dataloader\n",
        "# Turn train and test custom Dataset's into DataLoader's\n",
        "from torch.utils.data import DataLoader\n",
        "trainloader = DataLoader(dataset=data['train'], # use custom created train Dataset\n",
        "                                     batch_size=4, # how many samples per batch?\n",
        "                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
        "                                     shuffle=True) # shuffle the data?\n",
        "\n",
        "testloader = DataLoader(dataset=data['test'], # use custom created test Dataset\n",
        "                                    batch_size=4, \n",
        "                                    num_workers=0, \n",
        "                                    shuffle=False) # don't usually need to shuffle testing data\n",
        "\n",
        "train_data_size = len(trainloader.dataset)\n",
        "test_data_size = len(testloader.dataset)\n",
        "\n",
        "# DEFINE YOUR OWN MODEL\n",
        "\n",
        "model_ft = models.convnext_tiny(pretrained=True)\n",
        "num_ftrs = model_ft.classifier[2].in_features\n",
        "# Here the size of each output sample is set to 10.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "model_ft.classifier[2] = nn.Linear(num_ftrs, 4)\n",
        "\n",
        "#######################\n",
        "\n",
        "# 2. LOSS AND OPTIMIZER\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# 3. move the model to GPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "model_ft.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtwb9WnRzdIx",
        "outputId": "d635007c-191c-455b-f8bd-a26356659f75"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ConvNeXt(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2dNormActivation(\n",
              "      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
              "      (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Sequential(\n",
              "      (0): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
              "      )\n",
              "      (1): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
              "      )\n",
              "      (2): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=96, out_features=384, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=384, out_features=96, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
              "      )\n",
              "    )\n",
              "    (2): Sequential(\n",
              "      (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (3): Sequential(\n",
              "      (0): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
              "      )\n",
              "      (1): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
              "      )\n",
              "      (2): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=192, out_features=768, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=768, out_features=192, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
              "      )\n",
              "    )\n",
              "    (4): Sequential(\n",
              "      (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (5): Sequential(\n",
              "      (0): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
              "      )\n",
              "      (1): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
              "      )\n",
              "      (2): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
              "      )\n",
              "      (3): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
              "      )\n",
              "      (4): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
              "      )\n",
              "      (5): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
              "      )\n",
              "      (6): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
              "      )\n",
              "      (7): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
              "      )\n",
              "      (8): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
              "      )\n",
              "    )\n",
              "    (6): Sequential(\n",
              "      (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
              "      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
              "    )\n",
              "    (7): Sequential(\n",
              "      (0): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
              "      )\n",
              "      (1): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
              "      )\n",
              "      (2): CNBlock(\n",
              "        (block): Sequential(\n",
              "          (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
              "          (1): Permute()\n",
              "          (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (3): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (4): GELU(approximate=none)\n",
              "          (5): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (6): Permute()\n",
              "        )\n",
              "        (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "  (classifier): Sequential(\n",
              "    (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
              "    (1): Flatten(start_dim=1, end_dim=-1)\n",
              "    (2): Linear(in_features=768, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time # to calculate training time\n",
        "\n",
        "def train_and_validate(model, loss_criterion, optimizer, epochs=25):\n",
        "    '''\n",
        "    Function to train and validate\n",
        "    Parameters\n",
        "        :param model: Model to train and validate\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "        :param optimizer: Optimizer for computing gradients\n",
        "        :param epochs: Number of epochs (default=25)\n",
        "  \n",
        "    Returns\n",
        "        model: Trained Model with best validation accuracy\n",
        "        history: (dict object): Having training loss, accuracy and validation loss, accuracy\n",
        "    '''\n",
        "    \n",
        "    start = time.time()\n",
        "    history = []\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        epoch_start = time.time()\n",
        "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
        "        \n",
        "        # Set to training mode\n",
        "        model.train()\n",
        "        \n",
        "        # Loss and Accuracy within the epoch\n",
        "        train_loss = 0.0\n",
        "        train_acc = 0.0\n",
        "        \n",
        "        valid_loss = 0.0\n",
        "        valid_acc = 0.0\n",
        "        \n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Clean existing gradients\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "            \n",
        "            # Compute loss\n",
        "            loss = loss_criterion(outputs, labels)\n",
        "            \n",
        "            # Backpropagate the gradients\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "            \n",
        "            # Compute the total loss for the batch and add it to train_loss\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            \n",
        "            # Compute the accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "            \n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "            \n",
        "            # Compute total accuracy in the whole batch and add to train_acc\n",
        "            train_acc += acc.item() * inputs.size(0)\n",
        "            \n",
        "            #print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
        "\n",
        "            \n",
        "        # Validation - No gradient tracking needed\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # Set to evaluation mode\n",
        "            model.eval()\n",
        "\n",
        "            # Validation loop\n",
        "            for j, (inputs, labels) in enumerate(testloader):\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass - compute outputs on input data using the model\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_criterion(outputs, labels)\n",
        "\n",
        "                # Compute the total loss for the batch and add it to valid_loss\n",
        "                valid_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                ret, predictions = torch.max(outputs.data, 1)\n",
        "                correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "                # Convert correct_counts to float and then compute the mean\n",
        "                acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "                # Compute total accuracy in the whole batch and add to valid_acc\n",
        "                valid_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "                #print(\"Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "            \n",
        "        # Find average training loss and training accuracy\n",
        "        avg_train_loss = train_loss/train_data_size \n",
        "        avg_train_acc = train_acc/train_data_size\n",
        "\n",
        "        # Find average training loss and training accuracy\n",
        "        avg_test_loss = valid_loss/test_data_size \n",
        "        avg_test_acc = valid_acc/test_data_size\n",
        "\n",
        "        history.append([avg_train_loss, avg_test_loss, avg_train_acc, avg_test_acc])\n",
        "                \n",
        "        epoch_end = time.time()\n",
        "    \n",
        "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \\n\\t\\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s\".format(epoch, avg_train_loss, avg_train_acc*100, avg_test_loss, avg_test_acc*100, epoch_end-epoch_start))\n",
        "        \n",
        "        # Save if the model has best accuracy till now\n",
        "        torch.save(model, 'cifar10_model_'+str(epoch)+'.pt')\n",
        "            \n",
        "    return model, history"
      ],
      "metadata": {
        "id": "JL-nFfjL0rdK"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Train the model for 10 epochs\n",
        " \n",
        "num_epochs = 10\n",
        "trained_model, history = train_and_validate(model_ft, criterion, optimizer, num_epochs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTni60Rk1HkA",
        "outputId": "7be968d5-00b9-42f5-dcd1-41c1d0193ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Analyze the loss curve\n",
        "\n",
        "history = np.array(history)\n",
        "plt.plot(history[:,0:2])\n",
        "plt.legend(['Tr Loss', 'Val Loss'])\n",
        "plt.xlabel('Epoch Number')\n",
        "plt.ylabel('Loss')\n",
        "plt.ylim(0,3)\n",
        "# plt.savefig('cifar10_loss_curve.png')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ajHQxePMD3A0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "#######################################################\n",
        "#               Define Dataset Class\n",
        "#######################################################\n",
        "\n",
        "class fruitDataset(Dataset):\n",
        "    def __init__(self, image_paths, transform=False):\n",
        "        self.image_paths = image_paths\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_filepath = self.image_paths[idx]\n",
        "#         image = cv2.imread(image_filepath)\n",
        "#         image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = Image.open(image_filepath) # if using torchvision transforms\n",
        "        # print(image_filepath)\n",
        "        label = image_filepath.split('/')[-2]\n",
        "        # print(label)\n",
        "        label = class_to_idx[label]\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image) # if using torchvision transforms\n",
        "        \n",
        "        return image, label\n",
        "\n",
        "#######################################################\n",
        "#                  Create Dataset\n",
        "#######################################################\n",
        "\n",
        "train_dataset = fruitDataset(train_image_paths,image_transforms['train'])\n",
        "test_dataset = fruitDataset(test_image_paths,image_transforms['test'])\n",
        "\n"
      ],
      "metadata": {
        "id": "If9TuKjMD3rN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependencies\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "\n",
        "\n",
        "# function to convert the JavaScript object into an OpenCV image\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "\n",
        "  return bbox_bytes"
      ],
      "metadata": {
        "id": "9kcSM5Q9Em4c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)\n",
        "  \n",
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "metadata": {
        "id": "dIq9K3ghFbav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform=transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(size=224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "\n",
        "categories = ['abaya','kebaya']"
      ],
      "metadata": {
        "id": "D1kQlKXnFcJu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab.patches import cv2_imshow\n",
        "# start streaming video from webcam\n",
        "video_stream()\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "    js_reply = video_frame(label_html, bbox)\n",
        "    if not js_reply:\n",
        "        break\n",
        "\n",
        "    # convert JS response to OpenCV Image\n",
        "    frame = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "    #     rgb_image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    # Apply transforms to the input image.\n",
        "    # input_tensor = transform(frame)\n",
        "    input_tensor = transform(frame)\n",
        "    # Add the batch dimension.\n",
        "    input_batch = input_tensor.unsqueeze(0)\n",
        "    input_batch = input_batch.to(device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        output = model(input_batch)\n",
        "        end_time = time.time()\n",
        "    # Get the softmax probabilities.\n",
        "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "    # Check the top 5 categories that are predicted.\n",
        "    top5_prob, top5_catid = torch.topk(probabilities, 2)\n",
        "    \n",
        "    cv2.putText(frame, f\"{top5_prob[0].item()*100:.3f}%\", (15, (1)*30), \n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "    cv2.putText(frame, f\"{categories[top5_catid[0]]}\", (160, (1)*30), \n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                1, (0, 0, 255), 2, cv2.LINE_AA)\n",
        "    print(categories[top5_catid[0]], top5_prob[0].item())\n",
        "    cv2_imshow(frame)"
      ],
      "metadata": {
        "id": "vYC_Stf4Fl7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCy3b5888zut"
      },
      "source": [
        "**QUESTION 2** **[35 marks]**\n",
        "\n",
        "Firstly, watch this video:\n",
        "\n",
        "https://drive.google.com/file/d/1bsypahR7I3f_R3DXkfw_tf0BrbCHxE_O/view?usp=sharing\n",
        "\n",
        "This video shows an example of masked face recognition where the deep learning model is able to detect and classify your face even when wearing a face mask. Using the end-to-end object detection pipeline that you have learned, develop your own masked face recognition such that the model should recognize your face even on face mask while recognize other persons as \"others\".\n",
        "\n",
        "Deliverables for this question are:\n",
        "\n",
        "- the model file. Change the name to <your_name>.pt file (e.g. hasan.pt).\n",
        "- a short video (~10 secs) containing your face and your friends faces (for inference)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JEFekluPuIB-"
      },
      "outputs": [],
      "source": [
        "#clone YOLOv5 and \n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "%pip install -qr requirements.txt # install dependencies\n",
        "%pip install -q roboflow\n",
        "\n",
        "import torch\n",
        "import os\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(model_format=\"yolov5\", notebook=\"ultralytics\")\n",
        "\n",
        "# set up environment\n",
        "os.environ[\"DATASET_DIRECTORY\"] = \"/content/datasets\"\n",
        "\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"w5TbWlPoEWPllkL7EUAz\")\n",
        "project = rf.workspace(\"deep-learning-qbyqu\").project(\"hasriface\")\n",
        "dataset = project.version(2).download(\"yolov5\")\n",
        "\n",
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"w5TbWlPoEWPllkL7EUAz\")\n",
        "project = rf.workspace(\"deep-learning-qbyqu\").project(\"hasriface\")\n",
        "dataset = project.version(3).download(\"yolov5\")\n",
        "\n",
        "!python train.py --img 416 --batch 16 --epochs 150 --data {dataset.location}/data.yaml --weights yolov5s.pt --cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCdAPKRaAia3"
      },
      "outputs": [],
      "source": [
        "# Start tensorboard\n",
        "# Launch after you have started training\n",
        "# logs save in the folder \"runs\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs\n",
        "\n",
        "\n",
        "!python detect.py --weights runs/train/exp/weights/best.pt --img 416 --conf 0.1 --source {dataset.location}/test/images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQCF-JcIAiTM"
      },
      "outputs": [],
      "source": [
        "#display inference on ALL test images\n",
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for imageName in glob.glob('/content/yolov5/runs/detect/exp/*.jpg'): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "pytorch-yolo",
      "language": "python",
      "name": "pytorch-yolo"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2571619daaf741299643959f9ef95b54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c79360a00cf845dab3af24defc6b5302",
              "IPY_MODEL_de6008807b614603bf5109d77dd4c6c6",
              "IPY_MODEL_229a8c2ea51241b8a98ab8836c2b3d95"
            ],
            "layout": "IPY_MODEL_4275d136bc324d2cb2fcf4af49430363"
          }
        },
        "c79360a00cf845dab3af24defc6b5302": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52774906953d438e80ace61b87b2a8b9",
            "placeholder": "​",
            "style": "IPY_MODEL_ca8ef0cbddaf43ebb570caffa20c54ee",
            "value": "100%"
          }
        },
        "de6008807b614603bf5109d77dd4c6c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4c905b3a4c043429afa7e768b516019",
            "max": 46830571,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93a7e4d560e74cfc951de621d5e2daff",
            "value": 46830571
          }
        },
        "229a8c2ea51241b8a98ab8836c2b3d95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11e2b1269e1c41308a1d2545dd7c27d3",
            "placeholder": "​",
            "style": "IPY_MODEL_a96c6da3eca64b00b7e9266e2596a940",
            "value": " 44.7M/44.7M [00:00&lt;00:00, 60.1MB/s]"
          }
        },
        "4275d136bc324d2cb2fcf4af49430363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52774906953d438e80ace61b87b2a8b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca8ef0cbddaf43ebb570caffa20c54ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4c905b3a4c043429afa7e768b516019": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93a7e4d560e74cfc951de621d5e2daff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11e2b1269e1c41308a1d2545dd7c27d3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a96c6da3eca64b00b7e9266e2596a940": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}